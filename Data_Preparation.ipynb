{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 1: Data Preparation & Embedding\n",
    "Load Flickr-8k dataset and generate embeddings using CLIP model\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class Flickr8kDataProcessor:\n",
    "    def __init__(self, data_dir=\"data\", embeddings_dir=\"embeddings\"):\n",
    "        \"\"\"\n",
    "        Initialize the data processor for Flickr-8k dataset\n",
    "        \n",
    "        Args:\n",
    "            data_dir (str): Directory containing the dataset\n",
    "            embeddings_dir (str): Directory to store embeddings\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.embeddings_dir = embeddings_dir\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(embeddings_dir, exist_ok=True)\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        \n",
    "        # Load CLIP model\n",
    "        print(f\"Loading CLIP model on {self.device}...\")\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(\"CLIP model loaded successfully!\")\n",
    "        \n",
    "    def load_flickr8k_dataset(self):\n",
    "        \"\"\"\n",
    "        Load Flickr-8k dataset from Hugging Face datasets\n",
    "        If not available, provide instructions for manual download\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "            print(\"Loading Flickr-8k dataset from Hugging Face...\")\n",
    "            dataset = load_dataset(\"nlphuji/flickr8k\")\n",
    "            return dataset\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load dataset from Hugging Face: {e}\")\n",
    "            print(\"\\nManual dataset setup required:\")\n",
    "            print(\"1. Download Flickr-8k dataset from: https://www.kaggle.com/datasets/adityajn105/flickr8k\")\n",
    "            print(\"2. Extract to the 'data' directory\")\n",
    "            print(\"3. Ensure the following structure:\")\n",
    "            print(\"   data/\")\n",
    "            print(\"   ├── Images/\")\n",
    "            print(\"   └── captions.txt\")\n",
    "            return None\n",
    "    \n",
    "    def process_manual_dataset(self):\n",
    "        \"\"\"\n",
    "        Process manually downloaded Flickr-8k dataset\n",
    "        \"\"\"\n",
    "        images_dir = os.path.join(self.data_dir, \"Images\")\n",
    "        captions_file = os.path.join(self.data_dir, \"captions.txt\")\n",
    "        \n",
    "        if not os.path.exists(images_dir) or not os.path.exists(captions_file):\n",
    "            print(\"Manual dataset not found. Please download Flickr-8k dataset.\")\n",
    "            return None\n",
    "        \n",
    "        # Load captions\n",
    "        captions_df = pd.read_csv(captions_file)\n",
    "        \n",
    "        # Group by image filename\n",
    "        image_captions = {}\n",
    "        for _, row in captions_df.iterrows():\n",
    "            image_name = row['image']\n",
    "            caption = row['caption']\n",
    "            if image_name not in image_captions:\n",
    "                image_captions[image_name] = []\n",
    "            image_captions[image_name].append(caption)\n",
    "        \n",
    "        return image_captions, images_dir\n",
    "    \n",
    "    def generate_embeddings(self, dataset=None):\n",
    "        \"\"\"\n",
    "        Generate embeddings for images and text in the dataset\n",
    "        \n",
    "        Args:\n",
    "            dataset: Hugging Face dataset or None for manual dataset\n",
    "        \"\"\"\n",
    "        if dataset is not None:\n",
    "            return self._generate_embeddings_hf(dataset)\n",
    "        else:\n",
    "            return self._generate_embeddings_manual()\n",
    "    \n",
    "    def _generate_embeddings_hf(self, dataset):\n",
    "        \"\"\"Generate embeddings from Hugging Face dataset\"\"\"\n",
    "        print(\"Generating embeddings from Hugging Face dataset...\")\n",
    "        \n",
    "        image_embeddings = {}\n",
    "        text_embeddings = {}\n",
    "        image_text_pairs = []\n",
    "        \n",
    "        # Process training split\n",
    "        for item in tqdm(dataset['train'], desc=\"Processing training data\"):\n",
    "            image = item['image']\n",
    "            captions = item['captions']\n",
    "            \n",
    "            # Generate image embedding\n",
    "            with torch.no_grad():\n",
    "                inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                image_features = self.model.get_image_features(**inputs)\n",
    "                image_embedding = image_features.cpu().numpy().flatten()\n",
    "            \n",
    "            image_embeddings[item['image_id']] = image_embedding\n",
    "            \n",
    "            # Generate text embeddings for each caption\n",
    "            for i, caption in enumerate(captions):\n",
    "                with torch.no_grad():\n",
    "                    inputs = self.processor(text=caption, return_tensors=\"pt\", padding=True)\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    text_features = self.model.get_text_features(**inputs)\n",
    "                    text_embedding = text_features.cpu().numpy().flatten()\n",
    "                \n",
    "                text_id = f\"{item['image_id']}_caption_{i}\"\n",
    "                text_embeddings[text_id] = text_embedding\n",
    "                image_text_pairs.append({\n",
    "                    'image_id': item['image_id'],\n",
    "                    'text_id': text_id,\n",
    "                    'caption': caption,\n",
    "                    'image_embedding': image_embedding,\n",
    "                    'text_embedding': text_embedding\n",
    "                })\n",
    "        \n",
    "        return image_embeddings, text_embeddings, image_text_pairs\n",
    "    \n",
    "    def _generate_embeddings_manual(self):\n",
    "        \"\"\"Generate embeddings from manually downloaded dataset\"\"\"\n",
    "        print(\"Generating embeddings from manual dataset...\")\n",
    "        \n",
    "        result = self.process_manual_dataset()\n",
    "        if result is None:\n",
    "            return None, None, None\n",
    "        \n",
    "        image_captions, images_dir = result\n",
    "        \n",
    "        image_embeddings = {}\n",
    "        text_embeddings = {}\n",
    "        image_text_pairs = []\n",
    "        \n",
    "        for image_name in tqdm(image_captions.keys(), desc=\"Processing images\"):\n",
    "            image_path = os.path.join(images_dir, image_name)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load and preprocess image\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                \n",
    "                # Generate image embedding\n",
    "                with torch.no_grad():\n",
    "                    inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    image_features = self.model.get_image_features(**inputs)\n",
    "                    image_embedding = image_features.cpu().numpy().flatten()\n",
    "                \n",
    "                image_embeddings[image_name] = image_embedding\n",
    "                \n",
    "                # Generate text embeddings for each caption\n",
    "                captions = image_captions[image_name]\n",
    "                for i, caption in enumerate(captions):\n",
    "                    with torch.no_grad():\n",
    "                        inputs = self.processor(text=caption, return_tensors=\"pt\", padding=True)\n",
    "                        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                        text_features = self.model.get_text_features(**inputs)\n",
    "                        text_embedding = text_features.cpu().numpy().flatten()\n",
    "                    \n",
    "                    text_id = f\"{image_name}_caption_{i}\"\n",
    "                    text_embeddings[text_id] = text_embedding\n",
    "                    image_text_pairs.append({\n",
    "                        'image_id': image_name,\n",
    "                        'text_id': text_id,\n",
    "                        'caption': caption,\n",
    "                        'image_embedding': image_embedding,\n",
    "                        'text_embedding': text_embedding\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return image_embeddings, text_embeddings, image_text_pairs\n",
    "    \n",
    "    def save_embeddings(self, image_embeddings, text_embeddings, image_text_pairs):\n",
    "        \"\"\"\n",
    "        Save embeddings and metadata to disk\n",
    "        \n",
    "        Args:\n",
    "            image_embeddings (dict): Dictionary of image embeddings\n",
    "            text_embeddings (dict): Dictionary of text embeddings\n",
    "            image_text_pairs (list): List of image-text pairs with embeddings\n",
    "        \"\"\"\n",
    "        print(\"Saving embeddings...\")\n",
    "        \n",
    "        # Save image embeddings\n",
    "        with open(os.path.join(self.embeddings_dir, 'image_embeddings.pkl'), 'wb') as f:\n",
    "            pickle.dump(image_embeddings, f)\n",
    "        \n",
    "        # Save text embeddings\n",
    "        with open(os.path.join(self.embeddings_dir, 'text_embeddings.pkl'), 'wb') as f:\n",
    "            pickle.dump(text_embeddings, f)\n",
    "        \n",
    "        # Save image-text pairs\n",
    "        with open(os.path.join(self.embeddings_dir, 'image_text_pairs.pkl'), 'wb') as f:\n",
    "            pickle.dump(image_text_pairs, f)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'num_images': len(image_embeddings),\n",
    "            'num_texts': len(text_embeddings),\n",
    "            'num_pairs': len(image_text_pairs),\n",
    "            'embedding_dim': len(next(iter(image_embeddings.values()))) if image_embeddings else 0\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.embeddings_dir, 'metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"Embeddings saved successfully!\")\n",
    "        print(f\"Images: {metadata['num_images']}\")\n",
    "        print(f\"Texts: {metadata['num_texts']}\")\n",
    "        print(f\"Pairs: {metadata['num_pairs']}\")\n",
    "        print(f\"Embedding dimension: {metadata['embedding_dim']}\")\n",
    "    \n",
    "    def load_embeddings(self):\n",
    "        \"\"\"\n",
    "        Load saved embeddings from disk\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (image_embeddings, text_embeddings, image_text_pairs, metadata)\n",
    "        \"\"\"\n",
    "        print(\"Loading saved embeddings...\")\n",
    "        \n",
    "        try:\n",
    "            image_embeddings_path = os.path.join(self.embeddings_dir, 'image_embeddings.pkl')\n",
    "            text_embeddings_path = os.path.join(self.embeddings_dir, 'text_embeddings.pkl')\n",
    "            pairs_path = os.path.join(self.embeddings_dir, 'image_text_pairs.pkl')\n",
    "            metadata_path = os.path.join(self.embeddings_dir, 'metadata.json')\n",
    "\n",
    "            with open(image_embeddings_path, 'rb') as f:\n",
    "                image_embeddings = pickle.load(f)\n",
    "\n",
    "            with open(text_embeddings_path, 'rb') as f:\n",
    "                text_embeddings = pickle.load(f)\n",
    "\n",
    "            with open(pairs_path, 'rb') as f:\n",
    "                image_text_pairs = pickle.load(f)\n",
    "\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "\n",
    "            print(\"Embeddings loaded successfully!\")\n",
    "            return image_embeddings, text_embeddings, image_text_pairs, metadata\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"No saved embeddings found. Please run data preparation first.\")\n",
    "            return None, None, None, None\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load embeddings: {e}\")\n",
    "            print(\"Your embeddings may be corrupted or incomplete. Consider regenerating them by running:\")\n",
    "            print(\"  python src/data_preparation.py\")\n",
    "            return None, None, None, None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run data preparation\"\"\"\n",
    "    print(\"=== Flickr-8k Data Preparation & Embedding Generation ===\")\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = Flickr8kDataProcessor()\n",
    "    \n",
    "    # Check if embeddings already exist\n",
    "    if os.path.exists(os.path.join(processor.embeddings_dir, 'metadata.json')):\n",
    "        print(\"Found existing embeddings. Loading...\")\n",
    "        image_embeddings, text_embeddings, image_text_pairs, metadata = processor.load_embeddings()\n",
    "        if image_embeddings is not None:\n",
    "            print(\"Embeddings loaded successfully!\")\n",
    "            return\n",
    "    \n",
    "    # Try to load dataset from Hugging Face\n",
    "    dataset = processor.load_flickr8k_dataset()\n",
    "    \n",
    "    # Generate embeddings\n",
    "    image_embeddings, text_embeddings, image_text_pairs = processor.generate_embeddings(dataset)\n",
    "    \n",
    "    if image_embeddings is not None:\n",
    "        # Save embeddings\n",
    "        processor.save_embeddings(image_embeddings, text_embeddings, image_text_pairs)\n",
    "        print(\"Data preparation completed successfully!\")\n",
    "    else:\n",
    "        print(\"Failed to generate embeddings. Please check your dataset setup.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
