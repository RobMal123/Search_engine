{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Data Preparation & Embedding\n",
        "\n",
        "This notebook loads the provided dataset (images + captions), selects a pre-trained multimodal model (CLIP), generates vector embeddings for both text and images in batches, and saves the results to the `embeddings/` folder.\n",
        "\n",
        "- Dataset paths are preconfigured for this repository layout.\n",
        "- Uses Hugging Face Transformers `CLIPModel` and `AutoProcessor`.\n",
        "- Outputs: `image_embeddings.pkl`, `text_embeddings.pkl`, `image_text_pairs.pkl`, `metadata.json`.\n",
        "\n",
        "Run the cells top-to-bottom.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup & Imports\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import AutoProcessor, CLIPModel\n",
        "\n",
        "# Repository-relative paths\n",
        "REPO_ROOT = Path(\"..\").resolve().parent if (Path.cwd().name == \"notebooks\") else Path(\".\").resolve()\n",
        "DATA_DIR = REPO_ROOT / \"data\"\n",
        "IMAGES_DIR = DATA_DIR / \"Images\"\n",
        "CAPTIONS_PATH = DATA_DIR / \"captions.txt\"\n",
        "EMBED_DIR = REPO_ROOT / \"embeddings\"\n",
        "EMBED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Device selection\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Batch config\n",
        "IMAGE_BATCH_SIZE = 64\n",
        "TEXT_BATCH_SIZE = 256\n",
        "\n",
        "# Model config (can be changed to other CLIP variants)\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load captions and build image-text pairs\n",
        "\n",
        "def read_captions(captions_file: Path) -> Dict[str, str]:\n",
        "    pairs: Dict[str, str] = {}\n",
        "    if not captions_file.exists():\n",
        "        raise FileNotFoundError(f\"Captions file not found: {captions_file}\")\n",
        "    with captions_file.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            # Expect lines like: filename.jpg\\tcaption text\n",
        "            if \"\\t\" in line:\n",
        "                fname, caption = line.split(\"\\t\", 1)\n",
        "            elif \",\" in line:\n",
        "                # fallback: CSV-like\n",
        "                fname, caption = line.split(\",\", 1)\n",
        "            else:\n",
        "                # fallback: first space split\n",
        "                parts = line.split(\" \", 1)\n",
        "                if len(parts) < 2:\n",
        "                    continue\n",
        "                fname, caption = parts\n",
        "            pairs[fname.strip()] = caption.strip()\n",
        "    return pairs\n",
        "\n",
        "caption_map = read_captions(CAPTIONS_PATH)\n",
        "print(f\"Loaded {len(caption_map)} captions.\")\n",
        "\n",
        "# Filter image files that exist\n",
        "all_image_files = {p.name: p for p in IMAGES_DIR.glob(\"**/*.jpg\")}\n",
        "kept_filenames = [fn for fn in caption_map.keys() if fn in all_image_files]\n",
        "print(f\"Images with captions and present on disk: {len(kept_filenames)}\")\n",
        "\n",
        "image_text_pairs = [(fn, caption_map[fn]) for fn in kept_filenames]\n",
        "image_paths = [all_image_files[fn] for fn in kept_filenames]\n",
        "texts = [caption_map[fn] for fn in kept_filenames]\n",
        "\n",
        "# Quick peek\n",
        "for i in range(min(3, len(image_text_pairs))):\n",
        "    print(image_text_pairs[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and processor (CLIP)\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME)\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# Get embedding dimensions\n",
        "image_emb_dim = model.visual_projection.out_features\n",
        "text_emb_dim = model.text_projection.out_features\n",
        "print(f\"Image emb dim: {image_emb_dim}, Text emb dim: {text_emb_dim}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batched embedding generation helpers\n",
        "\n",
        "def chunked(iterable, n):\n",
        "    for i in range(0, len(iterable), n):\n",
        "        yield iterable[i:i+n]\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_image_embeddings(paths: List[Path]) -> torch.Tensor:\n",
        "    embs = []\n",
        "    for batch_paths in tqdm(list(chunked(paths, IMAGE_BATCH_SIZE)), desc=\"Images -> embeddings\"):\n",
        "        images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
        "        inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "        outputs = model.get_image_features(**inputs)\n",
        "        # Normalize to unit length as CLIP standard\n",
        "        outputs = outputs / outputs.norm(dim=-1, keepdim=True)\n",
        "        embs.append(outputs.cpu())\n",
        "    return torch.cat(embs, dim=0) if embs else torch.empty((0, image_emb_dim))\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_text_embeddings(texts: List[str]) -> torch.Tensor:\n",
        "    embs = []\n",
        "    for batch_texts in tqdm(list(chunked(texts, TEXT_BATCH_SIZE)), desc=\"Texts -> embeddings\"):\n",
        "        inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
        "        outputs = model.get_text_features(**inputs)\n",
        "        outputs = outputs / outputs.norm(dim=-1, keepdim=True)\n",
        "        embs.append(outputs.cpu())\n",
        "    return torch.cat(embs, dim=0) if embs else torch.empty((0, text_emb_dim))\n",
        "\n",
        "# Execute embedding computation\n",
        "image_embeddings = compute_image_embeddings(image_paths)\n",
        "text_embeddings = compute_text_embeddings(texts)\n",
        "\n",
        "assert image_embeddings.shape[0] == len(image_paths)\n",
        "assert text_embeddings.shape[0] == len(texts)\n",
        "print(\"Embeddings computed:\")\n",
        "print(\" - images:\", tuple(image_embeddings.shape))\n",
        "print(\" - texts:\", tuple(text_embeddings.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Persist embeddings and metadata\n",
        "\n",
        "# Convert to plain Python for pickling\n",
        "image_embeddings_np = image_embeddings.numpy()\n",
        "text_embeddings_np = text_embeddings.numpy()\n",
        "\n",
        "# Save embeddings\n",
        "with open(EMBED_DIR / \"image_embeddings.pkl\", \"wb\") as f:\n",
        "    pickle.dump(image_embeddings_np, f)\n",
        "with open(EMBED_DIR / \"text_embeddings.pkl\", \"wb\") as f:\n",
        "    pickle.dump(text_embeddings_np, f)\n",
        "\n",
        "# Save image-text pairs\n",
        "pairs = [(fn, cap) for fn, cap in image_text_pairs]\n",
        "with open(EMBED_DIR / \"image_text_pairs.pkl\", \"wb\") as f:\n",
        "    pickle.dump(pairs, f)\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"device\": DEVICE,\n",
        "    \"num_items\": len(pairs),\n",
        "    \"image_embedding_dim\": int(image_embeddings_np.shape[1]) if image_embeddings_np.size else 0,\n",
        "    \"text_embedding_dim\": int(text_embeddings_np.shape[1]) if text_embeddings_np.size else 0,\n",
        "}\n",
        "with open(EMBED_DIR / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"Saved embeddings and metadata to\", EMBED_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick verification\n",
        "\n",
        "print(\"Files in embeddings dir:\")\n",
        "for p in sorted(EMBED_DIR.glob(\"*\")):\n",
        "    print(\" -\", p.name, p.stat().st_size, \"bytes\")\n",
        "\n",
        "# Load a few to verify shapes\n",
        "with open(EMBED_DIR / \"image_embeddings.pkl\", \"rb\") as f:\n",
        "    img_embs = pickle.load(f)\n",
        "with open(EMBED_DIR / \"text_embeddings.pkl\", \"rb\") as f:\n",
        "    txt_embs = pickle.load(f)\n",
        "with open(EMBED_DIR / \"image_text_pairs.pkl\", \"rb\") as f:\n",
        "    pairs = pickle.load(f)\n",
        "\n",
        "print(\"Loaded shapes:\")\n",
        "print(\" - images:\", img_embs.shape)\n",
        "print(\" - texts:\", txt_embs.shape)\n",
        "print(\" - pairs:\", len(pairs))\n",
        "\n",
        "# Show an example\n",
        "if len(pairs):\n",
        "    print(\"Example:\")\n",
        "    print(pairs[0])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
